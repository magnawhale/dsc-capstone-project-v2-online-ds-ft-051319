{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Cycle Influence on Twitter Sentiment and Stock Markets\n",
    "\n",
    "article on lunar cycles & crime: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1444800/\n",
    "\n",
    "project proposal write-up: https://docs.google.com/document/d/1sAl3SyxhI6gLZ1zDEHhtIOfjB0FzofjjM3Bp7CaIBgM/edit#\n",
    "\n",
    "how to let web app access Twitter without exposing my keys: https://thurmancms.herokuapp.com/articles/how-to-add-twitter-access-token-and-secret-key-to-heroku-application\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitterscraper import query_tweets\n",
    "import twitterscraper\n",
    "# import datetime\n",
    "import csv\n",
    "import json\n",
    "import subprocess\n",
    "# import os\n",
    "import shutil\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain Scrub Explore Model iNterpret!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Twitter Data\n",
    "\n",
    "Twitter API documentation: https://developer.twitter.com/en/docs/api-reference-index\n",
    "\n",
    "instructions on accessing the Twitter API and performing sentiment analysis: https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/\n",
    "\n",
    "this one has more options and discusses removing user handles and more: https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/\n",
    "\n",
    "lunar phases: https://www.calendar-12.com/moon_phases/2018\n",
    "    \n",
    "- automating Twitter requests with python: https://github.com/Jefferson-Henrique/GetOldTweets-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet preprocessing & Sentiment analysis links\n",
    "\n",
    "   - good blog enter on sentiment analysis & preprocessing tweets: https://towardsdatascience.com/extracting-twitter-data-pre-processing-and-sentiment-analysis-using-python-3-0-7192bd8b47cf\n",
    "   - Python library that specifically cleans tweets: https://pypi.org/project/tweet-preprocessor/\n",
    "   - some more customizable options: https://stackoverflow.com/questions/8376691/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression \n",
    "\n",
    "to think about: do I want to keep happy/sad emojis? what about simply removing the '#' character from hashtags to retain text sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `twitterscraper` library for obtaining tweets. Documentation here: https://github.com/taspinar/twitterscraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using cmd line method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a list of dates for 2018\n",
    "dates_2018=[str(date)[:10] for date in pd.date_range(start='1/1/2018', periods=365)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a list of dates for generating file names\n",
    "dates_stripped_2018 = [date.replace('-','') for date in dates_2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scraping 0 days of 2018\n",
      "finished scraping 1 days of 2018\n",
      "finished scraping 2 days of 2018\n",
      "finished scraping 3 days of 2018\n",
      "finished scraping 4 days of 2018\n",
      "finished scraping 5 days of 2018\n",
      "finished scraping 6 days of 2018\n",
      "finished scraping 7 days of 2018\n",
      "finished scraping 8 days of 2018\n",
      "finished scraping 9 days of 2018\n",
      "finished scraping 10 days of 2018\n",
      "finished scraping 11 days of 2018\n",
      "finished scraping 12 days of 2018\n",
      "finished scraping 13 days of 2018\n",
      "finished scraping 14 days of 2018\n",
      "finished scraping 15 days of 2018\n",
      "finished scraping 16 days of 2018\n",
      "finished scraping 17 days of 2018\n",
      "finished scraping 18 days of 2018\n",
      "finished scraping 19 days of 2018\n",
      "finished scraping 20 days of 2018\n",
      "finished scraping 21 days of 2018\n",
      "finished scraping 22 days of 2018\n",
      "finished scraping 23 days of 2018\n",
      "finished scraping 24 days of 2018\n",
      "finished scraping 25 days of 2018\n",
      "finished scraping 26 days of 2018\n",
      "finished scraping 27 days of 2018\n",
      "finished scraping 28 days of 2018\n",
      "finished scraping 29 days of 2018\n",
      "finished scraping 30 days of 2018\n",
      "finished scraping 31 days of 2018\n",
      "finished scraping 32 days of 2018\n",
      "finished scraping 33 days of 2018\n",
      "finished scraping 34 days of 2018\n",
      "finished scraping 35 days of 2018\n",
      "finished scraping 36 days of 2018\n",
      "finished scraping 37 days of 2018\n",
      "finished scraping 38 days of 2018\n",
      "finished scraping 39 days of 2018\n",
      "finished scraping 40 days of 2018\n",
      "finished scraping 41 days of 2018\n",
      "finished scraping 42 days of 2018\n",
      "finished scraping 43 days of 2018\n",
      "finished scraping 44 days of 2018\n",
      "finished scraping 45 days of 2018\n",
      "finished scraping 46 days of 2018\n",
      "finished scraping 47 days of 2018\n",
      "finished scraping 48 days of 2018\n",
      "finished scraping 49 days of 2018\n",
      "finished scraping 50 days of 2018\n",
      "finished scraping 51 days of 2018\n",
      "finished scraping 52 days of 2018\n",
      "finished scraping 53 days of 2018\n",
      "finished scraping 54 days of 2018\n",
      "finished scraping 55 days of 2018\n",
      "finished scraping 56 days of 2018\n",
      "finished scraping 57 days of 2018\n",
      "finished scraping 58 days of 2018\n",
      "finished scraping 59 days of 2018\n",
      "finished scraping 60 days of 2018\n",
      "finished scraping 61 days of 2018\n",
      "finished scraping 62 days of 2018\n",
      "finished scraping 63 days of 2018\n",
      "finished scraping 64 days of 2018\n",
      "finished scraping 65 days of 2018\n",
      "finished scraping 66 days of 2018\n",
      "finished scraping 67 days of 2018\n",
      "finished scraping 68 days of 2018\n",
      "finished scraping 69 days of 2018\n",
      "finished scraping 70 days of 2018\n",
      "finished scraping 71 days of 2018\n",
      "finished scraping 72 days of 2018\n",
      "finished scraping 73 days of 2018\n",
      "finished scraping 74 days of 2018\n",
      "finished scraping 75 days of 2018\n",
      "finished scraping 76 days of 2018\n",
      "finished scraping 77 days of 2018\n",
      "finished scraping 78 days of 2018\n",
      "finished scraping 79 days of 2018\n",
      "finished scraping 80 days of 2018\n",
      "finished scraping 81 days of 2018\n",
      "finished scraping 82 days of 2018\n",
      "finished scraping 83 days of 2018\n",
      "finished scraping 84 days of 2018\n",
      "finished scraping 85 days of 2018\n",
      "finished scraping 86 days of 2018\n",
      "finished scraping 87 days of 2018\n",
      "finished scraping 88 days of 2018\n",
      "finished scraping 89 days of 2018\n",
      "finished scraping 90 days of 2018\n",
      "finished scraping 91 days of 2018\n",
      "finished scraping 92 days of 2018\n",
      "finished scraping 93 days of 2018\n",
      "finished scraping 94 days of 2018\n",
      "finished scraping 95 days of 2018\n",
      "finished scraping 96 days of 2018\n",
      "finished scraping 97 days of 2018\n",
      "finished scraping 98 days of 2018\n",
      "finished scraping 99 days of 2018\n",
      "finished scraping 100 days of 2018\n",
      "finished scraping 101 days of 2018\n",
      "finished scraping 102 days of 2018\n",
      "finished scraping 103 days of 2018\n",
      "finished scraping 104 days of 2018\n",
      "finished scraping 105 days of 2018\n",
      "finished scraping 106 days of 2018\n",
      "finished scraping 107 days of 2018\n",
      "finished scraping 108 days of 2018\n",
      "finished scraping 109 days of 2018\n",
      "finished scraping 110 days of 2018\n",
      "finished scraping 111 days of 2018\n",
      "finished scraping 112 days of 2018\n",
      "finished scraping 113 days of 2018\n",
      "finished scraping 114 days of 2018\n",
      "finished scraping 115 days of 2018\n",
      "finished scraping 116 days of 2018\n",
      "finished scraping 117 days of 2018\n",
      "finished scraping 118 days of 2018\n",
      "finished scraping 119 days of 2018\n",
      "finished scraping 120 days of 2018\n",
      "finished scraping 121 days of 2018\n",
      "finished scraping 122 days of 2018\n",
      "finished scraping 123 days of 2018\n",
      "finished scraping 124 days of 2018\n",
      "finished scraping 125 days of 2018\n",
      "finished scraping 126 days of 2018\n",
      "finished scraping 127 days of 2018\n",
      "finished scraping 128 days of 2018\n",
      "finished scraping 129 days of 2018\n",
      "finished scraping 130 days of 2018\n",
      "finished scraping 131 days of 2018\n",
      "finished scraping 132 days of 2018\n",
      "finished scraping 133 days of 2018\n",
      "finished scraping 134 days of 2018\n",
      "finished scraping 135 days of 2018\n",
      "finished scraping 136 days of 2018\n",
      "finished scraping 137 days of 2018\n",
      "finished scraping 138 days of 2018\n",
      "finished scraping 139 days of 2018\n",
      "finished scraping 140 days of 2018\n",
      "finished scraping 141 days of 2018\n",
      "finished scraping 142 days of 2018\n",
      "finished scraping 143 days of 2018\n",
      "finished scraping 144 days of 2018\n",
      "finished scraping 145 days of 2018\n",
      "finished scraping 146 days of 2018\n",
      "finished scraping 147 days of 2018\n",
      "finished scraping 148 days of 2018\n",
      "finished scraping 149 days of 2018\n",
      "finished scraping 150 days of 2018\n",
      "finished scraping 151 days of 2018\n",
      "finished scraping 152 days of 2018\n",
      "finished scraping 153 days of 2018\n",
      "finished scraping 154 days of 2018\n",
      "finished scraping 155 days of 2018\n",
      "finished scraping 156 days of 2018\n",
      "finished scraping 157 days of 2018\n",
      "finished scraping 158 days of 2018\n",
      "finished scraping 159 days of 2018\n",
      "finished scraping 160 days of 2018\n",
      "finished scraping 161 days of 2018\n",
      "finished scraping 162 days of 2018\n",
      "finished scraping 163 days of 2018\n",
      "finished scraping 164 days of 2018\n",
      "finished scraping 165 days of 2018\n",
      "finished scraping 166 days of 2018\n",
      "finished scraping 167 days of 2018\n",
      "finished scraping 168 days of 2018\n",
      "finished scraping 169 days of 2018\n",
      "finished scraping 170 days of 2018\n",
      "finished scraping 171 days of 2018\n",
      "finished scraping 172 days of 2018\n",
      "finished scraping 173 days of 2018\n",
      "finished scraping 174 days of 2018\n",
      "finished scraping 175 days of 2018\n",
      "finished scraping 176 days of 2018\n",
      "finished scraping 177 days of 2018\n",
      "finished scraping 178 days of 2018\n",
      "finished scraping 179 days of 2018\n",
      "finished scraping 180 days of 2018\n",
      "finished scraping 181 days of 2018\n",
      "finished scraping 182 days of 2018\n",
      "finished scraping 183 days of 2018\n",
      "finished scraping 184 days of 2018\n",
      "finished scraping 185 days of 2018\n",
      "finished scraping 186 days of 2018\n",
      "finished scraping 187 days of 2018\n",
      "finished scraping 188 days of 2018\n",
      "finished scraping 189 days of 2018\n",
      "finished scraping 190 days of 2018\n",
      "finished scraping 191 days of 2018\n",
      "finished scraping 192 days of 2018\n",
      "finished scraping 193 days of 2018\n",
      "finished scraping 194 days of 2018\n",
      "finished scraping 195 days of 2018\n",
      "finished scraping 196 days of 2018\n",
      "finished scraping 197 days of 2018\n",
      "finished scraping 198 days of 2018\n",
      "finished scraping 199 days of 2018\n",
      "finished scraping 200 days of 2018\n",
      "finished scraping 201 days of 2018\n",
      "finished scraping 202 days of 2018\n",
      "finished scraping 203 days of 2018\n",
      "finished scraping 204 days of 2018\n",
      "finished scraping 205 days of 2018\n",
      "finished scraping 206 days of 2018\n",
      "finished scraping 207 days of 2018\n",
      "finished scraping 208 days of 2018\n",
      "finished scraping 209 days of 2018\n",
      "finished scraping 210 days of 2018\n",
      "finished scraping 211 days of 2018\n",
      "finished scraping 212 days of 2018\n",
      "finished scraping 213 days of 2018\n",
      "finished scraping 214 days of 2018\n",
      "finished scraping 215 days of 2018\n",
      "finished scraping 216 days of 2018\n",
      "finished scraping 217 days of 2018\n",
      "finished scraping 218 days of 2018\n",
      "finished scraping 219 days of 2018\n",
      "finished scraping 220 days of 2018\n",
      "finished scraping 221 days of 2018\n",
      "finished scraping 222 days of 2018\n",
      "finished scraping 223 days of 2018\n",
      "finished scraping 224 days of 2018\n",
      "finished scraping 225 days of 2018\n",
      "finished scraping 226 days of 2018\n",
      "finished scraping 227 days of 2018\n",
      "finished scraping 228 days of 2018\n",
      "finished scraping 229 days of 2018\n",
      "finished scraping 230 days of 2018\n",
      "finished scraping 231 days of 2018\n",
      "finished scraping 232 days of 2018\n",
      "finished scraping 233 days of 2018\n",
      "finished scraping 234 days of 2018\n",
      "finished scraping 235 days of 2018\n",
      "finished scraping 236 days of 2018\n",
      "finished scraping 237 days of 2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scraping 238 days of 2018\n",
      "finished scraping 239 days of 2018\n",
      "finished scraping 240 days of 2018\n",
      "finished scraping 241 days of 2018\n",
      "finished scraping 242 days of 2018\n",
      "finished scraping 243 days of 2018\n",
      "finished scraping 244 days of 2018\n",
      "finished scraping 245 days of 2018\n",
      "finished scraping 246 days of 2018\n",
      "finished scraping 247 days of 2018\n",
      "finished scraping 248 days of 2018\n",
      "finished scraping 249 days of 2018\n",
      "finished scraping 250 days of 2018\n",
      "finished scraping 251 days of 2018\n",
      "finished scraping 252 days of 2018\n",
      "finished scraping 253 days of 2018\n",
      "finished scraping 254 days of 2018\n",
      "finished scraping 255 days of 2018\n",
      "finished scraping 256 days of 2018\n",
      "finished scraping 257 days of 2018\n",
      "finished scraping 258 days of 2018\n",
      "finished scraping 259 days of 2018\n",
      "finished scraping 260 days of 2018\n",
      "finished scraping 261 days of 2018\n",
      "finished scraping 262 days of 2018\n",
      "finished scraping 263 days of 2018\n",
      "finished scraping 264 days of 2018\n",
      "finished scraping 265 days of 2018\n",
      "finished scraping 266 days of 2018\n",
      "finished scraping 267 days of 2018\n",
      "finished scraping 268 days of 2018\n",
      "finished scraping 269 days of 2018\n",
      "finished scraping 270 days of 2018\n",
      "finished scraping 271 days of 2018\n",
      "finished scraping 272 days of 2018\n",
      "finished scraping 273 days of 2018\n",
      "finished scraping 274 days of 2018\n",
      "finished scraping 275 days of 2018\n",
      "finished scraping 276 days of 2018\n",
      "finished scraping 277 days of 2018\n",
      "finished scraping 278 days of 2018\n",
      "finished scraping 279 days of 2018\n",
      "finished scraping 280 days of 2018\n",
      "finished scraping 281 days of 2018\n",
      "finished scraping 282 days of 2018\n",
      "finished scraping 283 days of 2018\n",
      "finished scraping 284 days of 2018\n",
      "finished scraping 285 days of 2018\n",
      "finished scraping 286 days of 2018\n",
      "finished scraping 287 days of 2018\n",
      "finished scraping 288 days of 2018\n",
      "finished scraping 289 days of 2018\n",
      "finished scraping 290 days of 2018\n",
      "finished scraping 291 days of 2018\n",
      "finished scraping 292 days of 2018\n",
      "finished scraping 293 days of 2018\n",
      "finished scraping 294 days of 2018\n",
      "finished scraping 295 days of 2018\n",
      "finished scraping 296 days of 2018\n",
      "finished scraping 297 days of 2018\n",
      "finished scraping 298 days of 2018\n",
      "finished scraping 299 days of 2018\n",
      "finished scraping 300 days of 2018\n",
      "finished scraping 301 days of 2018\n",
      "finished scraping 302 days of 2018\n",
      "finished scraping 303 days of 2018\n",
      "finished scraping 304 days of 2018\n",
      "finished scraping 305 days of 2018\n",
      "finished scraping 306 days of 2018\n",
      "finished scraping 307 days of 2018\n",
      "finished scraping 308 days of 2018\n",
      "finished scraping 309 days of 2018\n",
      "finished scraping 310 days of 2018\n",
      "finished scraping 311 days of 2018\n",
      "finished scraping 312 days of 2018\n",
      "finished scraping 313 days of 2018\n",
      "finished scraping 314 days of 2018\n",
      "finished scraping 315 days of 2018\n",
      "finished scraping 316 days of 2018\n",
      "finished scraping 317 days of 2018\n",
      "finished scraping 318 days of 2018\n",
      "finished scraping 319 days of 2018\n",
      "finished scraping 320 days of 2018\n",
      "finished scraping 321 days of 2018\n",
      "finished scraping 322 days of 2018\n",
      "finished scraping 323 days of 2018\n",
      "finished scraping 324 days of 2018\n",
      "finished scraping 325 days of 2018\n",
      "finished scraping 326 days of 2018\n",
      "finished scraping 327 days of 2018\n",
      "finished scraping 328 days of 2018\n",
      "finished scraping 329 days of 2018\n",
      "finished scraping 330 days of 2018\n",
      "finished scraping 331 days of 2018\n",
      "finished scraping 332 days of 2018\n",
      "finished scraping 333 days of 2018\n",
      "finished scraping 334 days of 2018\n",
      "finished scraping 335 days of 2018\n",
      "finished scraping 336 days of 2018\n",
      "finished scraping 337 days of 2018\n",
      "finished scraping 338 days of 2018\n",
      "finished scraping 339 days of 2018\n",
      "finished scraping 340 days of 2018\n",
      "finished scraping 341 days of 2018\n",
      "finished scraping 342 days of 2018\n",
      "finished scraping 343 days of 2018\n",
      "finished scraping 344 days of 2018\n",
      "finished scraping 345 days of 2018\n",
      "finished scraping 346 days of 2018\n",
      "finished scraping 347 days of 2018\n",
      "finished scraping 348 days of 2018\n",
      "finished scraping 349 days of 2018\n",
      "finished scraping 350 days of 2018\n",
      "finished scraping 351 days of 2018\n",
      "finished scraping 352 days of 2018\n",
      "finished scraping 353 days of 2018\n",
      "finished scraping 354 days of 2018\n",
      "finished scraping 355 days of 2018\n",
      "finished scraping 356 days of 2018\n",
      "finished scraping 357 days of 2018\n",
      "finished scraping 358 days of 2018\n",
      "finished scraping 359 days of 2018\n",
      "finished scraping 360 days of 2018\n",
      "finished scraping 361 days of 2018\n",
      "finished scraping 362 days of 2018\n",
      "finished scraping 363 days of 2018\n",
      "finished scraping 364 days of 2018\n"
     ]
    }
   ],
   "source": [
    "# iterate scraping 1000 tweets from each day of 2018\n",
    "query = 'life OR death OR happy OR sad Exclude:retweets'\n",
    "for i in range(len(dates_2018)):\n",
    "    begin_date = dates_2018[i]\n",
    "    if i == 364:\n",
    "        end_date = '2019-01-01'\n",
    "    else:\n",
    "        end_date = dates_2018[i+1]\n",
    "    day = dates_stripped_2018[i]\n",
    "    cmd = 'twitterscraper \"{}\" -l 1000 -o t{}.json -bd {} -ed {} --lang en'.format(query, \n",
    "            day, begin_date, end_date)\n",
    "    subprocess.run(cmd)\n",
    "    \n",
    "    #move JSON file into `data` directory\n",
    "    shutil.move(f't{day}.json', f'./data/t{day}.json')\n",
    "    print(f\"finished scraping {i+1} days of 2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Financial Data\n",
    "\n",
    "website to get historical infra-day stock prices: https://www.alphavantage.co/documentation/\n",
    "   - request limitations: 5/minute, 500/day\n",
    "   - URL structure: https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=MSFT&outputsize=full&apikey=demo\n",
    "   - (back up option: https://docs.intrinio.com/tutorial/web_api)\n",
    "   \n",
    "I need to be careful about how I look at this data. Since stock markets are only open 6.5 hours a day, 5 days a week, certain lunar cycle days will fall on weekends, while considerable after-hours trading can affect next-day opening prices. Since lunar cycles are most noticeble at night, I should probably set stock days as 24-hour periods, from opening bell to following opening bell to account for evening trading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOW DO I HIDE my API keys from the public and still use the code???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT post this block of code publicly!!\n",
    "stocks_API_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enter the Stock Symbol for data request\n",
    "symbol = 'MSFT'\n",
    "\n",
    "# Making an API request for a certain stock's hsitory\n",
    "credentials = {'function':'TIME_SERIES_DAILY',\n",
    "               'symbol':symbol,\n",
    "               'outputsize':'full',\n",
    "               'apikey':stocks_API_key}\n",
    "r = requests.get('https://www.alphavantage.co/query', params=credentials)\n",
    "\n",
    "# checking to make sure request was successful\n",
    "print(r.status_code)\n",
    "if r.status_code == requests.codes.ok:\n",
    "    print(\"Request Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cleaning up the data to make it easier to work with\n",
    "MSFTdf = pd.DataFrame(r.json()[\"Time Series (Daily)\"])\n",
    "MSFTdf = MSFTdf.T.reset_index()\n",
    "MSFTdf.columns = ['date','open','high','low','close','volume']\n",
    "MSFTdf.date = pd.to_datetime(MSFTdf.date)\n",
    "MSFTdf[['open','high','low','close','volume']] = MSFTdf[['open',\n",
    "    'high','low','close','volume']].astype(float)\n",
    "\n",
    "# create a new column to account for after-hours trading\n",
    "# this uses the next day's open value as the prior day's close value\n",
    "cl24 = [MSFTdf.loc[0].close]\n",
    "for val in MSFTdf.open.values:\n",
    "    cl24.append(val)\n",
    "cl24 = pd.DataFrame(cl24[:-1], columns=['close_24'])\n",
    "MSFTdf = MSFTdf.join(cl24)\n",
    "\n",
    "# setting date column as index to facilitate timeseries manipulation\n",
    "MSFTdf.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_24</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>88.200</td>\n",
       "      <td>88.58</td>\n",
       "      <td>87.605</td>\n",
       "      <td>88.28</td>\n",
       "      <td>22113000.0</td>\n",
       "      <td>88.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>87.660</td>\n",
       "      <td>88.41</td>\n",
       "      <td>87.430</td>\n",
       "      <td>88.19</td>\n",
       "      <td>23407100.0</td>\n",
       "      <td>88.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>86.590</td>\n",
       "      <td>87.66</td>\n",
       "      <td>86.570</td>\n",
       "      <td>87.11</td>\n",
       "      <td>21912000.0</td>\n",
       "      <td>87.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>86.055</td>\n",
       "      <td>86.51</td>\n",
       "      <td>85.970</td>\n",
       "      <td>86.35</td>\n",
       "      <td>26061400.0</td>\n",
       "      <td>86.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>86.125</td>\n",
       "      <td>86.31</td>\n",
       "      <td>85.500</td>\n",
       "      <td>85.95</td>\n",
       "      <td>22483800.0</td>\n",
       "      <td>86.055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              open   high     low  close      volume  close_24\n",
       "date                                                          \n",
       "2018-01-08  88.200  88.58  87.605  88.28  22113000.0    88.650\n",
       "2018-01-05  87.660  88.41  87.430  88.19  23407100.0    88.200\n",
       "2018-01-04  86.590  87.66  86.570  87.11  21912000.0    87.660\n",
       "2018-01-03  86.055  86.51  85.970  86.35  26061400.0    86.590\n",
       "2018-01-02  86.125  86.31  85.500  85.95  22483800.0    86.055"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting only 2018 data\n",
    "MSFT_2018 = MSFTdf['2018':'2018']\n",
    "MSFT_2018.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 251 entries, 2018-12-31 to 2018-01-02\n",
      "Data columns (total 6 columns):\n",
      "open        251 non-null float64\n",
      "high        251 non-null float64\n",
      "low         251 non-null float64\n",
      "close       251 non-null float64\n",
      "volume      251 non-null float64\n",
      "close_24    251 non-null float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 13.7 KB\n"
     ]
    }
   ],
   "source": [
    "MSFT_2018.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's also see how Bitcoin performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entering the Currency Symbols\n",
    "symbol = 'BTC'\n",
    "\n",
    "# Making an API request for a certain stock's hsitory\n",
    "credentials = {'function':'DIGITAL_CURRENCY_DAILY', \n",
    "               'symbol':symbol, \n",
    "               'market':'USD', \n",
    "               'apikey':stocks_API_key}\n",
    "r = requests.get('https://www.alphavantage.co/query', params=credentials)\n",
    "\n",
    "# checking to make sure request was successful\n",
    "print(r.status_code)\n",
    "if r.status_code == requests.codes.ok:\n",
    "    print(\"Request Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_24</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>3803.12</td>\n",
       "      <td>3810.00</td>\n",
       "      <td>3630.33</td>\n",
       "      <td>3702.90</td>\n",
       "      <td>29991.778350</td>\n",
       "      <td>3701.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-30</th>\n",
       "      <td>3696.71</td>\n",
       "      <td>3903.50</td>\n",
       "      <td>3657.90</td>\n",
       "      <td>3801.91</td>\n",
       "      <td>33222.369262</td>\n",
       "      <td>3803.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-29</th>\n",
       "      <td>3839.00</td>\n",
       "      <td>3892.00</td>\n",
       "      <td>3670.00</td>\n",
       "      <td>3695.32</td>\n",
       "      <td>38874.373903</td>\n",
       "      <td>3696.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>3567.89</td>\n",
       "      <td>3887.25</td>\n",
       "      <td>3540.04</td>\n",
       "      <td>3839.26</td>\n",
       "      <td>45964.304987</td>\n",
       "      <td>3839.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>3777.74</td>\n",
       "      <td>3813.98</td>\n",
       "      <td>3535.00</td>\n",
       "      <td>3567.91</td>\n",
       "      <td>44097.392912</td>\n",
       "      <td>3567.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               open     high      low    close        volume  close_24\n",
       "date                                                                  \n",
       "2018-12-31  3803.12  3810.00  3630.33  3702.90  29991.778350   3701.23\n",
       "2018-12-30  3696.71  3903.50  3657.90  3801.91  33222.369262   3803.12\n",
       "2018-12-29  3839.00  3892.00  3670.00  3695.32  38874.373903   3696.71\n",
       "2018-12-28  3567.89  3887.25  3540.04  3839.26  45964.304987   3839.00\n",
       "2018-12-27  3777.74  3813.98  3535.00  3567.91  44097.392912   3567.89"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 365 entries, 2018-12-31 to 2018-01-01\n",
      "Data columns (total 6 columns):\n",
      "open        365 non-null float64\n",
      "high        365 non-null float64\n",
      "low         365 non-null float64\n",
      "close       365 non-null float64\n",
      "volume      365 non-null float64\n",
      "close_24    365 non-null float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 20.0 KB\n"
     ]
    }
   ],
   "source": [
    "# cleaning up the data to make it easier to work with\n",
    "BTCdf = pd.DataFrame(r.json()[\"Time Series (Digital Currency Daily)\"])\n",
    "BTCdf = BTCdf.T.reset_index()\n",
    "BTCdf.drop(columns=[\"1b. open (USD)\",\"2b. high (USD)\",\"3b. low (USD)\",\n",
    "    \"4b. close (USD)\",\"6. market cap (USD)\"], inplace=True, axis=1)\n",
    "BTCdf.columns = ['date','open','high','low','close','volume']\n",
    "BTCdf.date = pd.to_datetime(BTCdf.date)\n",
    "BTCdf[['open','high','low','close','volume']] = BTCdf[['open',\n",
    "    'high','low','close','volume']].astype(float)\n",
    "\n",
    "# create a new column to account for after-hours trading\n",
    "# this uses the next day's open value as the prior day's close value\n",
    "cl24 = [BTCdf.loc[0].close]\n",
    "for val in BTCdf.open.values:\n",
    "    cl24.append(val)\n",
    "cl24 = pd.DataFrame(cl24[:-1], columns=['close_24'])\n",
    "BTCdf = BTCdf.join(cl24)\n",
    "\n",
    "# setting date column as index to facilitate timeseries manipulation\n",
    "BTCdf.set_index('date', inplace=True)\n",
    "\n",
    "# selecting only 2018 data\n",
    "BTC_2018 = BTCdf['2018':'2018']\n",
    "display(BTC_2018.head())\n",
    "BTC_2018.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And also the S&P 500 Index\n",
    "\n",
    "The S&P 500 is an average of the performance of 500 large companies, so we can use this to approximate the entire stock market. (It is one of the most commonly used indicators of the overall US stock market)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Request Successful\n"
     ]
    }
   ],
   "source": [
    "# Entering the Stock Symbol\n",
    "symbol = 'INX'\n",
    "\n",
    "# Making an API request for a certain stock's hsitory\n",
    "credentials = {'function':'TIME_SERIES_DAILY',\n",
    "               'symbol':symbol,\n",
    "               'outputsize':'full',\n",
    "               'apikey':stocks_API_key}\n",
    "r = requests.get('https://www.alphavantage.co/query', params=credentials)\n",
    "\n",
    "# checking to make sure request was successful\n",
    "print(r.status_code)\n",
    "if r.status_code == requests.codes.ok:\n",
    "    print(\"Request Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_24</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>2498.9399</td>\n",
       "      <td>2509.2400</td>\n",
       "      <td>2482.8201</td>\n",
       "      <td>2506.8501</td>\n",
       "      <td>3.442870e+09</td>\n",
       "      <td>2476.9600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>2498.7700</td>\n",
       "      <td>2520.2700</td>\n",
       "      <td>2472.8899</td>\n",
       "      <td>2485.7400</td>\n",
       "      <td>3.702620e+09</td>\n",
       "      <td>2498.9399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>2442.5000</td>\n",
       "      <td>2489.1001</td>\n",
       "      <td>2397.9399</td>\n",
       "      <td>2488.8301</td>\n",
       "      <td>4.096610e+09</td>\n",
       "      <td>2498.7700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-26</th>\n",
       "      <td>2363.1201</td>\n",
       "      <td>2467.7600</td>\n",
       "      <td>2346.5801</td>\n",
       "      <td>2467.7000</td>\n",
       "      <td>4.233990e+09</td>\n",
       "      <td>2442.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-24</th>\n",
       "      <td>2400.5601</td>\n",
       "      <td>2410.3401</td>\n",
       "      <td>2351.1001</td>\n",
       "      <td>2351.1001</td>\n",
       "      <td>2.613930e+09</td>\n",
       "      <td>2363.1201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 open       high        low      close        volume  \\\n",
       "date                                                                   \n",
       "2018-12-31  2498.9399  2509.2400  2482.8201  2506.8501  3.442870e+09   \n",
       "2018-12-28  2498.7700  2520.2700  2472.8899  2485.7400  3.702620e+09   \n",
       "2018-12-27  2442.5000  2489.1001  2397.9399  2488.8301  4.096610e+09   \n",
       "2018-12-26  2363.1201  2467.7600  2346.5801  2467.7000  4.233990e+09   \n",
       "2018-12-24  2400.5601  2410.3401  2351.1001  2351.1001  2.613930e+09   \n",
       "\n",
       "             close_24  \n",
       "date                   \n",
       "2018-12-31  2476.9600  \n",
       "2018-12-28  2498.9399  \n",
       "2018-12-27  2498.7700  \n",
       "2018-12-26  2442.5000  \n",
       "2018-12-24  2363.1201  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 251 entries, 2018-12-31 to 2018-01-02\n",
      "Data columns (total 6 columns):\n",
      "open        251 non-null float64\n",
      "high        251 non-null float64\n",
      "low         251 non-null float64\n",
      "close       251 non-null float64\n",
      "volume      251 non-null float64\n",
      "close_24    251 non-null float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 13.7 KB\n"
     ]
    }
   ],
   "source": [
    "# cleaning up the data to make it easier to work with\n",
    "SP500df = pd.DataFrame(r.json()[\"Time Series (Daily)\"])\n",
    "SP500df = SP500df.T.reset_index()\n",
    "SP500df.columns = ['date','open','high','low','close','volume']\n",
    "SP500df.date = pd.to_datetime(SP500df.date)\n",
    "SP500df[['open','high','low','close','volume']] = SP500df[['open',\n",
    "    'high','low','close','volume']].astype(float)\n",
    "\n",
    "# create a new column to account for after-hours trading\n",
    "# this uses the next day's open value as the prior day's close value\n",
    "cl24 = [SP500df.loc[0].close]\n",
    "for val in SP500df.open.values:\n",
    "    cl24.append(val)\n",
    "cl24 = pd.DataFrame(cl24[:-1], columns=['close_24'])\n",
    "SP500df = SP500df.join(cl24)\n",
    "\n",
    "# setting date column as index to facilitate timeseries manipulation\n",
    "SP500df.set_index('date', inplace=True)\n",
    "\n",
    "# selecting only 2018 data\n",
    "SP500_2018 = SP500df['2018':'2018']\n",
    "display(SP500_2018.head())\n",
    "SP500_2018.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining List of Dates of 2018 Lunar Phases\n",
    "\n",
    "File is stored as `lunar_phases_2018.csv`, adapted from https://www.calendar-12.com/moon_phases/2018 . Times/dates are calculated from U.S. Eastern Time Zone (using Daylight/Standard time as appropriate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>date</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Full Moon</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Mon</td>\n",
       "      <td>20:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Last Quarter</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>Mon</td>\n",
       "      <td>16:26:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Moon</td>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>Tue</td>\n",
       "      <td>20:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First Quarter</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>Wed</td>\n",
       "      <td>16:20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Full Moon</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>Wed</td>\n",
       "      <td>7:27:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           phase       date weekday      time\n",
       "0      Full Moon 2018-01-01     Mon  20:25:00\n",
       "1   Last Quarter 2018-01-08     Mon  16:26:00\n",
       "2       New Moon 2018-01-16     Tue  20:18:00\n",
       "3  First Quarter 2018-01-24     Wed  16:20:00\n",
       "4      Full Moon 2018-01-31     Wed   7:27:00"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading in a list of 2018 lunar phases\n",
    "phases_2018 = pd.read_csv('lunar_phases_2018.csv')\n",
    "phases_2018.date = pd.to_datetime(phases_2018.date)\n",
    "phases_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrubbing the Data\n",
    "\n",
    "Thankfully, the stock market data is already completely cleaned. The twitter data, however, still requires pretty extensive scrubbing to remove odd characters, links, and usernames. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIRST I MUST PROCESS THE TWEETS\n",
    "\n",
    "CSVwriter doesn't understand emojis and some other special characters, so I need to remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode characters in position 78-101: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-fc2973cd1ce9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mcsvwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"timestamp\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# close('tweets_2018.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\learn-env\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode characters in position 78-101: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "### make a .CSV file that compiles the relevant info from the JSONs\n",
    "\n",
    "# create the csv writer object\n",
    "csvwriter = csv.writer(open('tweets_2018.csv', 'w'))\n",
    "csvwriter.writerow([\"timestamp\", \"text\"])\n",
    "\n",
    "# iterate adding rows of JSON to the CSV file\n",
    "for i in dates_stripped_2018:\n",
    "    f = open(f'./data/t{i}.json')\n",
    "    data = json.load(f)\n",
    "    for tweet in data:\n",
    "        csvwriter.writerow([tweet[\"timestamp\"], tweet[\"text\"]])\n",
    "    f.close()\n",
    "# close('tweets_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close('tweets_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to clean tweets and get tweet sentiment\n",
    "from textblob import TextBlob \n",
    "\n",
    "def clean_tweet(tweet): \n",
    "    ''' \n",
    "    Utility function to clean tweet text by removing links, usernames, and\n",
    "    special characters using simple regex statements. \n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) \\\n",
    "                            |(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n",
    "\n",
    "\n",
    "def get_tweet_sentiment(tweet): \n",
    "    ''' \n",
    "    Utility function to classify sentiment of passed tweet \n",
    "    using textblob's sentiment method \n",
    "    '''\n",
    "    # create TextBlob object of passed tweet text \n",
    "    analysis = TextBlob(clean_tweet(tweet)) \n",
    "    # set sentiment \n",
    "    if analysis.sentiment.polarity > 0: \n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0: \n",
    "        return 'neutral'\n",
    "    else: \n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis code below comes from:\n",
    "# https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/\n",
    "\n",
    "# import re \n",
    "# import tweepy \n",
    "from tweepy import OAuthHandler \n",
    "from textblob import TextBlob \n",
    "  \n",
    "class TwitterClient(object): \n",
    "    ''' \n",
    "    Generic Twitter Class for sentiment analysis. \n",
    "    '''\n",
    "\n",
    "    def __init__(self): \n",
    "        ''' \n",
    "        Class constructor or initialization method. \n",
    "        '''\n",
    "        # keys and tokens from the Twitter Dev Console \n",
    "        consumer_key = 'XXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "        consumer_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "#         access_token = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "#         access_token_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "  \n",
    "        # attempt authentication \n",
    "        try: \n",
    "            # create OAuthHandler object \n",
    "            self.auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "#             # set access token and secret \n",
    "#             self.auth.set_access_token(access_token, access_token_secret) \n",
    "            # create tweepy API object to fetch tweets \n",
    "            self.api = tweepy.API(self.auth) \n",
    "        except: \n",
    "            print(\"Error: Authentication Failed\") \n",
    "\n",
    "            \n",
    "    def clean_tweet(self, tweet): \n",
    "        ''' \n",
    "        Utility function to clean tweet text by removing links, usernames, and\n",
    "        special characters using simple regex statements. \n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) \\\n",
    "                                |(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n",
    "\n",
    "    \n",
    "    def get_tweet_sentiment(self, tweet): \n",
    "        ''' \n",
    "        Utility function to classify sentiment of passed tweet \n",
    "        using textblob's sentiment method \n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text \n",
    "        analysis = TextBlob(self.clean_tweet(tweet)) \n",
    "        # set sentiment \n",
    "        if analysis.sentiment.polarity > 0: \n",
    "            return 'positive'\n",
    "        elif analysis.sentiment.polarity == 0: \n",
    "            return 'neutral'\n",
    "        else: \n",
    "            return 'negative'\n",
    "\n",
    "        \n",
    "    def get_tweets(self, query, count=1000): \n",
    "        ''' \n",
    "        Main function to fetch tweets and parse them. \n",
    "        '''\n",
    "        # empty list to store parsed tweets \n",
    "        tweets = [] \n",
    "  \n",
    "        try: \n",
    "            # call twitter api to fetch tweets \n",
    "            fetched_tweets = self.api.search(q = query, count = count) \n",
    "  \n",
    "            # parsing tweets one by one \n",
    "            for tweet in fetched_tweets: \n",
    "                # empty dictionary to store required params of a tweet \n",
    "                parsed_tweet = {} \n",
    "  \n",
    "                # saving text of tweet \n",
    "                parsed_tweet['text'] = tweet.text \n",
    "                # saving sentiment of tweet \n",
    "                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text) \n",
    "  \n",
    "                # appending parsed tweet to tweets list \n",
    "                if tweet.retweet_count > 0: \n",
    "                    # if tweet has retweets, ensure that it is appended only once \n",
    "                    if parsed_tweet not in tweets: \n",
    "                        tweets.append(parsed_tweet) \n",
    "                else: \n",
    "                    tweets.append(parsed_tweet) \n",
    "  \n",
    "            # return parsed tweets \n",
    "            return tweets \n",
    "  \n",
    "        except tweepy.TweepError as e: \n",
    "            # print error (if any) \n",
    "            print(\"Error : \" + str(e)) \n",
    "\n",
    "\n",
    "def main(query, count=1000): \n",
    "    # creating object of TwitterClient Class \n",
    "    api = TwitterClient() \n",
    "    # calling function to get tweets \n",
    "    tweets = api.get_tweets(query = query, count = count) \n",
    "  \n",
    "    # picking positive tweets from tweets \n",
    "    ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive'] \n",
    "    # percentage of positive tweets \n",
    "    print(f\"Positive tweets percentage: {100*len(ptweets)/len(tweets)} %\") \n",
    "    # picking negative tweets from tweets \n",
    "    ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative'] \n",
    "    # percentage of negative tweets \n",
    "    print(f\"Negative tweets percentage: {100*len(ntweets)/len(tweets)} %\") \n",
    "    # percentage of neutral tweets \n",
    "    print(f\"Neutral tweets percentage: {100*len(tweets - ntweets - ptweets)/len(tweets)} %\") \n",
    "  \n",
    "    # printing first 5 positive tweets \n",
    "    print(\"\\n\\nPositive tweets:\") \n",
    "    for tweet in ptweets[:10]: \n",
    "        print(tweet['text']) \n",
    "  \n",
    "    # printing first 5 negative tweets \n",
    "    print(\"\\n\\nNegative tweets:\") \n",
    "    for tweet in ntweets[:10]: \n",
    "        print(tweet['text']) \n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\": \n",
    "#     # calling main function \n",
    "#     main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
