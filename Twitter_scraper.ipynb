{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is solely for Twitter scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: {'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; rv:2.2) Gecko/20110201'}\n"
     ]
    }
   ],
   "source": [
    "# for scraping the web\n",
    "import requests\n",
    "from twitterscraper import query_tweets\n",
    "import twitterscraper\n",
    "\n",
    "# file management\n",
    "import csv\n",
    "import json\n",
    "import subprocess\n",
    "# import os\n",
    "import shutil\n",
    "from textblob import TextBlob \n",
    "# import preprocessor as p          #is this the right one?    I want this one: https://pypi.org/project/tweet-preprocessor/\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining several functions to automate scraping and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dates_list(year=2018):\n",
    "    # define a list of dates for a given year\n",
    "    dates_year =[str(date)[:10] for date in pd.date_range(start=f'1/1/{year}', end=f'12/31/{year}')]\n",
    "    # define a list of dates for generating file names\n",
    "    dates_stripped_year = [date.replace('-','') for date in dates_year]\n",
    "    return dates_year, dates_stripped_year\n",
    "\n",
    "\n",
    "def scrape_tweets(query, year=2018, num_tweets=1000):\n",
    "    \"\"\"\n",
    "    Automatically scrapes X number of Tweets per day for a given\n",
    "    year. Scraping works backwards from midnight.\n",
    "    'query' must be a string\n",
    "    \"\"\"\n",
    "    dates_year, dates_stripped_year = make_dates_list(year)\n",
    "    for i in range(len(dates_year)):\n",
    "        begin_date = dates_year[i]\n",
    "        if i == len(dates_year)-1:\n",
    "            end_date = f'{year+1}-01-01'\n",
    "        else:\n",
    "            end_date = dates_year[i+1]\n",
    "        day = dates_stripped_year[i]\n",
    "        cmd = 'twitterscraper \"{}\" -l {} -o t{}.json -bd {} -ed {} --lang en'.format(query, \n",
    "                num_tweets, day, begin_date, end_date)\n",
    "        subprocess.run(cmd)\n",
    "\n",
    "        #move JSON file into `data` directory\n",
    "        shutil.move(f't{day}.json', f'./data/t{day}.json')\n",
    "        if (i+1)%5 == 0:\n",
    "            print(f\"finished scraping {i+1} days of {year}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define sets of old-school emoticons and emojis\n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to clean tweets and get tweet sentiment\n",
    "\n",
    "# portions of the code below comes from :\n",
    "# https://towardsdatascience.com/extracting-twitter-data-pre-processing-and-sentiment-analysis-using-python-3-0-7192bd8b47cf\n",
    "def replace_emoticons(tweet):\n",
    "    \"This code replaces happy and sad emoticons with the words 'HAPPY' and 'SAD'\"\n",
    "    rhappy = '[' + re.escape(''.join(emoticons_happy)) + ']'\n",
    "    re.sub(rhappy, ' HAPPY ', tweet)\n",
    "    rsad = '[' + re.escape(''.join(emoticons_sad)) + ']'\n",
    "    re.sub(rsad, ' SAD ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def clean_tweet(tweet): \n",
    "    ''' \n",
    "    Utility function to clean tweet text by removing links, usernames, and\n",
    "    special characters using simple regex statements. \n",
    "    '''\n",
    "    tweet = replace_emoticons(tweet)\n",
    "    # p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION)\n",
    "    # tweet = p.clean(tweet)\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) \\\n",
    "                            |(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    return tweet\n",
    "\n",
    "def double_clean_tweet(tweet):\n",
    "    \"This function goes a little further than the previous clean function\"\n",
    "    #removing mentions\n",
    "    tweet = re.sub(r':', ' ', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', ' ', tweet)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    #remove emojis from tweet  (unless you want to later go through the UNICODE\n",
    "    # charts and separate \"happy\" emojis from \"sad\" emojis and add them to \n",
    "    # the `replace_emoticons()` function)\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "# Sentiment analysis code below adapted from:\n",
    "# https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/\n",
    "def get_tweet_sentiment(tweet): \n",
    "    ''' \n",
    "    Utility function to classify sentiment of passed tweet \n",
    "    using textblob's sentiment method \n",
    "    '''\n",
    "    # create TextBlob object of passed tweet text \n",
    "    analysis = TextBlob(tweet)\n",
    "    # set sentiment \n",
    "    polarity = analysis.sentiment.polarity\n",
    "    subjectivity = analysis.sentiment.subjectivity\n",
    "    if analysis.sentiment.polarity > 0.1: \n",
    "        sentiment = 'positive'\n",
    "    elif analysis.sentiment.polarity < -0.1: \n",
    "        sentiment = 'negative'\n",
    "    else: \n",
    "        sentiment = 'neutral'\n",
    "    return sentiment, polarity, subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to create a .CSV file that compiles the relevant \n",
    "# info from the JSONs, preprocesses the tweets, and performs sentiment analysis\n",
    "def json_to_csv_tweets(output_filename='output.csv', year=2018):\n",
    "    \"\"\"\n",
    "    Takes in JSON files of scraped tweets from the `./data/` folder,\n",
    "    cleans the tweets, performs sentiment analysis, and then outputs\n",
    "    the results to the provided destination CSV filename.\n",
    "    \"\"\"\n",
    "    # create the csv writer object\n",
    "    csvwriter = csv.writer(open(output_filename, 'w', newline=''))\n",
    "    csvwriter.writerow([\"timestamp\", \"text\", \"sentiment\", \"polarity\", \"subjectivity\", \"tally\"])\n",
    "\n",
    "    # iterate adding rows of JSON to the CSV file\n",
    "    year_length = len([date for date in pd.date_range(start=f'1/1/{year}', end=f'12/31/{year}')])\n",
    "    for i in year_length:\n",
    "        f = open(f'./data/t{i}.json')\n",
    "        data = json.load(f)\n",
    "        for tweet in data:\n",
    "            tw = tweet[\"text\"]\n",
    "            tw = replace_emoticons(tw)\n",
    "            tw = clean_tweet(tw)\n",
    "            tw = double_clean_tweet(tw)\n",
    "            sentiment, polarity, subjectivity = get_tweet_sentiment(tw)\n",
    "            csvwriter.writerow([i, tw, sentiment, polarity, subjectivity, 1])\n",
    "        f.close()\n",
    "        if float(i)%20 == 0:\n",
    "            print(f\"Finished working with:   ./data/t{i}.json\")\n",
    "    print(\"JOB IS COMPLETELY FINISHED.  HOORAY!!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'stocks OR money OR taxes Exclude:retweets'\n",
    "scrape_tweets(query=query, year=2018, num_tweets=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_to_csv_tweets('tweets_moneywords_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if scraper worked on all dates\n",
    "# a few missing dates is not the end of the world\n",
    "grouped1 = pd.DataFrame(tw_df.groupby(['timestamp'])['tally'].sum())\n",
    "grouped1 = grouped1.sort_values(by=['tally'])\n",
    "grouped1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
